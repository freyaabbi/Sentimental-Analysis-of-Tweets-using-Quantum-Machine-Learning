{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qiskit==0.39.2Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading qiskit-0.39.2.tar.gz (13 kB)\n",
      "Collecting qiskit-terra==0.22.2\n",
      "  Downloading qiskit_terra-0.22.2-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "Collecting qiskit-aer==0.11.1\n",
      "  Downloading qiskit_aer-0.11.1-cp310-cp310-win_amd64.whl (24.3 MB)\n",
      "Collecting qiskit-ibmq-provider==0.19.2\n",
      "  Downloading qiskit_ibmq_provider-0.19.2-py3-none-any.whl (240 kB)\n",
      "Collecting scipy>=1.0\n",
      "  Downloading scipy-1.14.1-cp310-cp310-win_amd64.whl (44.8 MB)\n",
      "Collecting numpy>=1.16.3\n",
      "  Downloading numpy-2.1.3-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Collecting websocket-client>=1.0.1\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Collecting requests-ntlm>=1.1.0\n",
      "  Downloading requests_ntlm-1.3.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting websockets>=10.0\n",
      "  Downloading websockets-14.1-cp310-cp310-win_amd64.whl (163 kB)\n",
      "Collecting requests>=2.19\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting urllib3>=1.21.1\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from qiskit-ibmq-provider==0.19.2->qiskit==0.39.2) (2.9.0.post0)\n",
      "Collecting tweedledum<2.0,>=1.1\n",
      "  Downloading tweedledum-1.1.1-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "Collecting dill>=0.3\n",
      "  Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "Collecting sympy>=1.3\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Requirement already satisfied: psutil>=5 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from qiskit-terra==0.22.2->qiskit==0.39.2) (6.1.0)\n",
      "Collecting ply>=3.10\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "Collecting stevedore>=3.0.0\n",
      "  Downloading stevedore-5.3.0-py3-none-any.whl (49 kB)\n",
      "Collecting retworkx>=0.11.0\n",
      "  Downloading retworkx-0.15.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.0->qiskit-ibmq-provider==0.19.2->qiskit==0.39.2) (1.16.0)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.0-cp310-cp310-win_amd64.whl (102 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting cryptography>=1.3\n",
      "  Downloading cryptography-43.0.3-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "Collecting pyspnego>=0.4.0\n",
      "  Downloading pyspnego-0.11.2-py3-none-any.whl (130 kB)\n",
      "Collecting cffi>=1.12\n",
      "  Downloading cffi-1.17.1-cp310-cp310-win_amd64.whl (181 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Collecting sspilib>=0.1.0\n",
      "  Downloading sspilib-0.2.0-cp310-cp310-win_amd64.whl (565 kB)\n",
      "Collecting rustworkx==0.15.1\n",
      "  Downloading rustworkx-0.15.1-cp38-abi3-win_amd64.whl (1.8 MB)\n",
      "Collecting pbr>=2.0.0\n",
      "  Downloading pbr-6.1.0-py2.py3-none-any.whl (108 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using legacy 'setup.py install' for qiskit, since package 'wheel' is not installed.\n",
      "Installing collected packages: pycparser, numpy, cffi, urllib3, sspilib, rustworkx, pbr, mpmath, idna, cryptography, charset-normalizer, certifi, tweedledum, sympy, stevedore, scipy, retworkx, requests, pyspnego, ply, dill, websockets, websocket-client, requests-ntlm, qiskit-terra, qiskit-ibmq-provider, qiskit-aer, qiskit\n",
      "    Running setup.py install for qiskit: started\n",
      "    Running setup.py install for qiskit: finished with status 'done'\n",
      "Successfully installed certifi-2024.8.30 cffi-1.17.1 charset-normalizer-3.4.0 cryptography-43.0.3 dill-0.3.9 idna-3.10 mpmath-1.3.0 numpy-2.1.3 pbr-6.1.0 ply-3.11 pycparser-2.22 pyspnego-0.11.2 qiskit-0.39.2 qiskit-aer-0.11.1 qiskit-ibmq-provider-0.19.2 qiskit-terra-0.22.2 requests-2.32.3 requests-ntlm-1.3.0 retworkx-0.15.1 rustworkx-0.15.1 scipy-1.14.1 sspilib-0.2.0 stevedore-5.3.0 sympy-1.13.3 tweedledum-1.1.1 urllib3-2.2.3 websocket-client-1.8.0 websockets-14.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pbr.exe is installed in 'c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script isympy.exe is installed in 'c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pyspnego-parse.exe is installed in 'c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script wsdump.exe is installed in 'c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 21.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qiskit-aer in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.16.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qiskit-aer) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qiskit-aer) (1.14.1)\n",
      "Requirement already satisfied: qiskit-terra>=0.21.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qiskit-aer) (0.22.2)\n",
      "Requirement already satisfied: sympy>=1.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qiskit-terra>=0.21.0->qiskit-aer) (1.13.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from qiskit-terra>=0.21.0->qiskit-aer) (2.9.0.post0)\n",
      "Requirement already satisfied: ply>=3.10 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qiskit-terra>=0.21.0->qiskit-aer) (3.11)\n",
      "Requirement already satisfied: dill>=0.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qiskit-terra>=0.21.0->qiskit-aer) (0.3.9)\n",
      "Requirement already satisfied: tweedledum<2.0,>=1.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qiskit-terra>=0.21.0->qiskit-aer) (1.1.1)\n",
      "Requirement already satisfied: retworkx>=0.11.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qiskit-terra>=0.21.0->qiskit-aer) (0.15.1)\n",
      "Requirement already satisfied: psutil>=5 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from qiskit-terra>=0.21.0->qiskit-aer) (6.1.0)\n",
      "Requirement already satisfied: stevedore>=3.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qiskit-terra>=0.21.0->qiskit-aer) (5.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.0->qiskit-terra>=0.21.0->qiskit-aer) (1.16.0)\n",
      "Requirement already satisfied: rustworkx==0.15.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from retworkx>=0.11.0->qiskit-terra>=0.21.0->qiskit-aer) (0.15.1)\n",
      "Requirement already satisfied: pbr>=2.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stevedore>=3.0.0->qiskit-terra>=0.21.0->qiskit-aer) (6.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.3->qiskit-terra>=0.21.0->qiskit-aer) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install qiskit==0.39.2\n",
    "%pip install qiskit-aer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22.2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'AerSimulator' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(qiskit\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqiskit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproviders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AerSimulator\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mAerSimulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'AerSimulator' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import qiskit\n",
    "print(qiskit.__version__)\n",
    "\n",
    "from qiskit.providers.aer import AerSimulator\n",
    "print(AerSimulator.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.providers.aer import AerSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-win_amd64.whl (11.0 MB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "import numpy as np\n",
    "import qiskit\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit.primitives import Sampler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from qiskit.visualization import plot_bloch_multivector\n",
    "from qiskit.quantum_info import Statevector\n",
    "from qiskit import QuantumCircuit, Aer, transpile\n",
    "from qiskit.circuit import ParameterVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit, Aer, transpile\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit.visualization import plot_bloch_multivector\n",
    "from qiskit.quantum_info import Statevector\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated quantum statevector: Statevector([0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j, 0.04224026+0.j, 0.04224026+0.j,\n",
      "             0.04224026+0.j],\n",
      "            dims=(2, 2, 2, 2, 2, 2))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.providers.aer import AerSimulator\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r'C:\\Users\\dell\\Desktop\\twitter.csv')\n",
    "\n",
    "# Step 1: Calculate tweet length from the fourth column\n",
    "df['tweet_length'] = df.iloc[:, 3].astype(str).apply(len)\n",
    "\n",
    "# Step 2: Normalize the tweet length\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(df[['tweet_length']])\n",
    "\n",
    "# Step 3: Select a subset of data (first 64 entries) to fit into 6 qubits\n",
    "subset_data = normalized_data[:64].flatten()  # Flatten to 1D array\n",
    "num_qubits = int(np.log2(len(subset_data)))   # Calculate required qubits\n",
    "\n",
    "# Step 4: Normalize the data for amplitude encoding\n",
    "norm = np.linalg.norm(subset_data)\n",
    "normalized_amplitude_data = subset_data / norm\n",
    "\n",
    "# Step 5: Create the quantum circuit with 6 qubits\n",
    "qc = QuantumCircuit(num_qubits)\n",
    "qc.initialize(normalized_amplitude_data, qc.qubits)  # Amplitude encoding\n",
    "\n",
    "# Save the statevector in the quantum circuit\n",
    "qc.save_statevector()\n",
    "\n",
    "# Step 6: Run the quantum circuit on a simulator\n",
    "simulator = AerSimulator()\n",
    "result = simulator.run(qc).result()\n",
    "\n",
    "# Retrieve the statevector\n",
    "statevector = result.get_statevector(qc)\n",
    "print(\"Simulated quantum statevector:\", statevector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values in 'Tweet_Text' with an empty string\n",
    "data['Tweet_Text'] = data['Tweet_Text'].fillna('')\n",
    "\n",
    "# Now, re-run the TF-IDF vectorizer\n",
    "X = vectorizer.fit_transform(data['Tweet_Text']).toarray()\n",
    "y = data['Sentiment'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pennylane in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.39.0)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pennylane) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from pennylane) (4.12.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from pennylane) (24.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pennylane) (1.14.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pennylane) (3.4.2)\n",
      "Requirement already satisfied: appdirs in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: autograd in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pennylane) (1.7.0)\n",
      "Requirement already satisfied: autoray>=0.6.11 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pennylane) (0.7.0)\n",
      "Requirement already satisfied: cachetools in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pennylane) (5.5.0)\n",
      "Requirement already satisfied: pennylane-lightning>=0.39 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pennylane) (0.39.0)\n",
      "Requirement already satisfied: numpy<2.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pennylane) (2.0.2)\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pennylane) (0.15.1)\n",
      "Requirement already satisfied: toml in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pennylane) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pennylane) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pennylane) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pennylane) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting execution at: 01:48:30\n",
      "Loading dataset...\n",
      "Using 1000 samples\n",
      "Vectorizing text...\n",
      "\n",
      "Training quantum classifier...\n",
      "Epoch 1/10, Loss: 6.3132, Time: 1.09s\n",
      "Epoch 2/10, Loss: 4.6328, Time: 0.78s\n",
      "Epoch 3/10, Loss: 4.2424, Time: 0.90s\n",
      "Epoch 4/10, Loss: 3.7120, Time: 0.93s\n",
      "Epoch 5/10, Loss: 3.4490, Time: 1.05s\n",
      "Epoch 6/10, Loss: 2.4355, Time: 1.06s\n",
      "Epoch 7/10, Loss: 2.1926, Time: 0.78s\n",
      "Epoch 8/10, Loss: 2.3599, Time: 0.91s\n",
      "Epoch 9/10, Loss: 2.4871, Time: 0.82s\n",
      "Epoch 10/10, Loss: 2.4166, Time: 0.94s\n",
      "\n",
      "Total training time: 9.27 seconds\n",
      "\n",
      "Evaluating model...\n",
      "Test Accuracy: 18.00%\n",
      "\n",
      "Example predictions (first 5 test samples):\n",
      "True: 1, Predicted: 1, Raw Score: 0.9792\n",
      "True: 3, Predicted: 1, Raw Score: 0.9792\n",
      "True: 3, Predicted: 1, Raw Score: 0.9792\n",
      "True: 3, Predicted: 1, Raw Score: 0.6618\n",
      "True: 3, Predicted: 1, Raw Score: 0.9792\n",
      "\n",
      "Execution completed at: 01:48:42\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Print start time\n",
    "print(\"Starting execution at:\", time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "print(\"Loading dataset...\")\n",
    "data = pd.read_csv(r'C:\\Users\\dell\\Desktop\\twitter.csv')\n",
    "data.columns = ['ID', 'Keyword', 'Sentiment', 'Tweet_Text']\n",
    "data = data[['Sentiment', 'Tweet_Text']]\n",
    "\n",
    "# Take a smaller subset for faster execution\n",
    "data = data.head(1000)  # Using only first 1000 rows\n",
    "print(f\"Using {len(data)} samples\")\n",
    "\n",
    "# Clean the data\n",
    "data['Tweet_Text'] = data['Tweet_Text'].fillna('')\n",
    "data = data.dropna(subset=['Sentiment'])\n",
    "\n",
    "# Encode labels to binary\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sentiment'] = label_encoder.fit_transform(data['Sentiment'])\n",
    "\n",
    "# Text vectorization using TF-IDF\n",
    "print(\"Vectorizing text...\")\n",
    "vectorizer = TfidfVectorizer(max_features=4)\n",
    "X = vectorizer.fit_transform(data['Tweet_Text'].astype(str)).toarray()\n",
    "y = data['Sentiment'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Quantum device setup\n",
    "n_qubits = 4\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "# Define the quantum circuit\n",
    "@qml.qnode(dev)\n",
    "def quantum_circuit(inputs, weights):\n",
    "    # Normalize inputs\n",
    "    inputs = inputs / (np.linalg.norm(inputs) + 1e-10)\n",
    "    \n",
    "    # Encode the input features\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(inputs[i], wires=i)\n",
    "    \n",
    "    # Apply trainable rotation gates\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(weights[i], wires=i)\n",
    "    \n",
    "    # Add entanglement\n",
    "    for i in range(n_qubits-1):\n",
    "        qml.CNOT(wires=[i, i+1])\n",
    "    \n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# Define the classical processing with batching\n",
    "def quantum_classifier(inputs, weights):\n",
    "    predictions = []\n",
    "    # Process in smaller batches\n",
    "    batch_size = 32\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch = inputs[i:i + batch_size]\n",
    "        batch_predictions = np.array([quantum_circuit(x, weights) for x in batch])\n",
    "        predictions.extend(batch_predictions)\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Simplified cost function\n",
    "def cost(weights, X, y):\n",
    "    predictions = quantum_classifier(X, weights)\n",
    "    return np.mean((y - predictions) ** 2)\n",
    "\n",
    "# Initialize weights\n",
    "np.random.seed(42)\n",
    "weights = np.random.uniform(0, 2*np.pi, size=n_qubits)\n",
    "\n",
    "# Training settings\n",
    "opt = qml.GradientDescentOptimizer(stepsize=0.1)\n",
    "batch_size = 32  # Increased batch size\n",
    "epochs = 10  # Reduced epochs\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nTraining quantum classifier...\")\n",
    "training_start = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Select random batch\n",
    "    batch_index = np.random.randint(0, len(X_train), (batch_size,))\n",
    "    X_batch = X_train[batch_index]\n",
    "    y_batch = y_train[batch_index]\n",
    "    \n",
    "    # Update the weights\n",
    "    weights = opt.step(lambda w: cost(w, X_batch, y_batch), weights)\n",
    "    \n",
    "    # Calculate loss for this epoch\n",
    "    loss = cost(weights, X_batch, y_batch)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "training_time = time.time() - training_start\n",
    "print(f\"\\nTotal training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nEvaluating model...\")\n",
    "predictions = quantum_classifier(X_test, weights)\n",
    "binary_predictions = (predictions > 0.0).astype(int)\n",
    "accuracy = accuracy_score(y_test, binary_predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print example predictions\n",
    "print(\"\\nExample predictions (first 5 test samples):\")\n",
    "for i in range(min(5, len(X_test))):\n",
    "    pred = quantum_classifier(X_test[i:i+1], weights)[0]\n",
    "    binary_pred = int(pred > 0.0)\n",
    "    print(f\"True: {y_test[i]}, Predicted: {binary_pred}, Raw Score: {pred:.4f}\")\n",
    "\n",
    "print(\"\\nExecution completed at:\", time.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK not found. Installing NLTK...\n",
      "Successfully installed nltk\n"
     ]
    }
   ],
   "source": [
    "# Check and install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"Successfully installed {package}\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"Failed to install {package}. Please install it manually using 'pip install {package}'\")\n",
    "\n",
    "# Check and install NLTK\n",
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    print(\"NLTK not found. Installing NLTK...\")\n",
    "    install_package('nltk')\n",
    "    import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn not found. Installing...\n",
      "Successfully installed scikit-learn\n"
     ]
    }
   ],
   "source": [
    "required_packages = ['pennylane', 'scikit-learn', 'pandas', 'nltk']\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"{package} not found. Installing...\")\n",
    "        install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punkt already downloaded\n",
      "Downloading stopwords...\n",
      "Downloading wordnet...\n",
      "Error setting up NLTK: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mwordnet\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\dell/nltk_data'\n",
      "    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\dell\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Attempting alternative download method...\n",
      "Alternative download successful\n"
     ]
    }
   ],
   "source": [
    "# Download and verify NLTK data\n",
    "def setup_nltk():\n",
    "    try:\n",
    "        # Create a default directory for NLTK data\n",
    "        import os\n",
    "        nltk_data_dir = os.path.join(os.path.expanduser(\"~\"), \"nltk_data\")\n",
    "        os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "        \n",
    "        # Download required NLTK data\n",
    "        required_packages = ['punkt', 'stopwords', 'wordnet']\n",
    "        for package in required_packages:\n",
    "            try:\n",
    "                nltk.data.find(f'tokenizers/{package}')\n",
    "                print(f\"{package} already downloaded\")\n",
    "            except LookupError:\n",
    "                print(f\"Downloading {package}...\")\n",
    "                nltk.download(package, download_dir=nltk_data_dir, quiet=True)\n",
    "        \n",
    "        # Verify the downloads\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "        nltk.data.find('corpora/wordnet')\n",
    "        print(\"All NLTK data packages successfully downloaded and verified\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up NLTK: {str(e)}\")\n",
    "        print(\"Attempting alternative download method...\")\n",
    "        try:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "            print(\"Alternative download successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download NLTK data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Call the setup function before preprocessing\n",
    "setup_nltk()\n",
    "\n",
    "# Modified preprocess_text function with better error handling\n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        try:\n",
    "            # Tokenize\n",
    "            tokens = word_tokenize(text)\n",
    "            # Remove stopwords and lemmatize\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "            return ' '.join(tokens)\n",
    "        except LookupError as e:\n",
    "            print(f\"NLTK data lookup error: {str(e)}\")\n",
    "            # Fallback to basic preprocessing if NLTK fails\n",
    "            return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocess_text: {str(e)}\")\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\dell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\dell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "except Exception as e:\n",
    "    print(\"Error downloading NLTK resources:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\dell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\dell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting execution at: 02:13:54\n",
      "Loading dataset...\n",
      "Cleaning and preprocessing data...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\dell/nltk_data'\n    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\dell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet_Text\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Apply text preprocessing\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet_Text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTweet_Text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Remove empty texts after preprocessing\u001b[39;00m\n\u001b[0;32m     53\u001b[0m data \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet_Text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[2], line 32\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     30\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Tokenize\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Remove stopwords and lemmatize\u001b[39;00m\n\u001b[0;32m     34\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\dell/nltk_data'\n    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\dell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "except:\n",
    "    print(\"NLTK data already downloaded or offline\")\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Starting execution at:\", time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "print(\"Loading dataset...\")\n",
    "data = pd.read_csv(r'C:\\Users\\dell\\Desktop\\twitter.csv')\n",
    "data.columns = ['ID', 'Keyword', 'Sentiment', 'Tweet_Text']\n",
    "\n",
    "# Enhanced data cleaning\n",
    "print(\"Cleaning and preprocessing data...\")\n",
    "# Remove rows with missing values\n",
    "data = data.dropna(subset=['Tweet_Text', 'Sentiment'])\n",
    "# Apply text preprocessing\n",
    "data['Tweet_Text'] = data['Tweet_Text'].apply(preprocess_text)\n",
    "# Remove empty texts after preprocessing\n",
    "data = data[data['Tweet_Text'].str.len() > 0]\n",
    "\n",
    "# Balance the dataset\n",
    "print(\"Balancing dataset...\")\n",
    "sentiment_counts = data['Sentiment'].value_counts()\n",
    "min_count = min(sentiment_counts)\n",
    "balanced_data = pd.DataFrame()\n",
    "for sentiment in sentiment_counts.index:\n",
    "    sentiment_data = data[data['Sentiment'] == sentiment]\n",
    "    balanced_data = pd.concat([balanced_data, sentiment_data.sample(n=min_count, random_state=42)])\n",
    "\n",
    "data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"Using {len(data)} balanced samples\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sentiment'] = label_encoder.fit_transform(data['Sentiment'])\n",
    "\n",
    "# Enhanced text vectorization\n",
    "print(\"Vectorizing text...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=8,  # Increased features\n",
    "    ngram_range=(1, 3),  # Added trigrams\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    stop_words='english',\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True  # Apply sublinear scaling\n",
    ")\n",
    "X = vectorizer.fit_transform(data['Tweet_Text']).toarray()\n",
    "y = data['Sentiment'].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Quantum device setup\n",
    "n_qubits = 8  # Increased number of qubits\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "# Enhanced quantum circuit with more layers\n",
    "@qml.qnode(dev)\n",
    "def quantum_circuit(inputs, weights):\n",
    "    # Normalize inputs\n",
    "    inputs = inputs / (np.linalg.norm(inputs) + 1e-10)\n",
    "    \n",
    "    # First rotation layer\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(inputs[i % len(inputs)], wires=i)\n",
    "        qml.RY(weights[i], wires=i)\n",
    "        qml.RZ(weights[i + n_qubits], wires=i)\n",
    "    \n",
    "    # First entanglement layer\n",
    "    for i in range(n_qubits-1):\n",
    "        qml.CNOT(wires=[i, i+1])\n",
    "    qml.CNOT(wires=[n_qubits-1, 0])\n",
    "    \n",
    "    # Second rotation layer\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(weights[i + 2*n_qubits], wires=i)\n",
    "        qml.RY(weights[i + 3*n_qubits], wires=i)\n",
    "        qml.RZ(weights[i + 4*n_qubits], wires=i)\n",
    "    \n",
    "    # Second entanglement layer\n",
    "    for i in range(0, n_qubits-1, 2):\n",
    "        qml.CNOT(wires=[i, i+1])\n",
    "    \n",
    "    # Final rotation layer\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(weights[i + 5*n_qubits], wires=i)\n",
    "        qml.RY(weights[i + 6*n_qubits], wires=i)\n",
    "    \n",
    "    # Measure all qubits and take average\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "# Enhanced classical processing\n",
    "def quantum_classifier(inputs, weights):\n",
    "    predictions = []\n",
    "    batch_size = 16  # Smaller batch size for more frequent updates\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch = inputs[i:i + batch_size]\n",
    "        batch_predictions = np.array([np.mean(quantum_circuit(x, weights)) for x in batch])\n",
    "        predictions.extend(batch_predictions)\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Enhanced cost function\n",
    "def cost(weights, X, y, lambda_reg=0.01):\n",
    "    predictions = quantum_classifier(X, weights)\n",
    "    epsilon = 1e-10\n",
    "    predictions = np.clip((predictions + 1) / 2, epsilon, 1 - epsilon)\n",
    "    # Weighted binary cross-entropy\n",
    "    pos_weight = np.sum(y == 0) / np.sum(y == 1)\n",
    "    loss = -np.mean(pos_weight * y * np.log(predictions) + \n",
    "                   (1 - y) * np.log(1 - predictions))\n",
    "    # L2 regularization\n",
    "    reg_term = lambda_reg * np.sum(weights**2)\n",
    "    return loss + reg_term\n",
    "\n",
    "# Initialize weights\n",
    "np.random.seed(42)\n",
    "n_weights = 7 * n_qubits  # 7 rotation layers\n",
    "weights = np.random.uniform(-0.1, 0.1, size=n_weights)\n",
    "\n",
    "# Training settings\n",
    "opt = qml.AdamOptimizer(stepsize=0.01, beta1=0.9, beta2=0.999)\n",
    "batch_size = 16\n",
    "epochs = 50  # Increased epochs\n",
    "\n",
    "# Training loop with enhanced early stopping\n",
    "print(\"\\nTraining quantum classifier...\")\n",
    "training_start = time.time()\n",
    "best_loss = float('inf')\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "best_weights = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Shuffle training data\n",
    "    shuffle_idx = np.random.permutation(len(X_train))\n",
    "    X_train_shuffled = X_train[shuffle_idx]\n",
    "    y_train_shuffled = y_train[shuffle_idx]\n",
    "    \n",
    "    # Mini-batch training\n",
    "    epoch_losses = []\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train_shuffled[i:i + batch_size]\n",
    "        y_batch = y_train_shuffled[i:i + batch_size]\n",
    "        \n",
    "        weights = opt.step(lambda w: cost(w, X_batch, y_batch), weights)\n",
    "        batch_loss = cost(weights, X_batch, y_batch)\n",
    "        epoch_losses.append(batch_loss)\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Early stopping with best weights saving\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_weights = weights.copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # Validate current accuracy\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        current_preds = quantum_classifier(X_test, weights)\n",
    "        current_preds = (current_preds > 0).astype(int)\n",
    "        current_acc = accuracy_score(y_test, current_preds)\n",
    "        print(f\"Current accuracy: {current_acc * 100:.2f}%\")\n",
    "        \n",
    "        if current_acc > 0.8:  # Early stopping if accuracy > 80%\n",
    "            print(\"Reached target accuracy!\")\n",
    "            break\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "# Use best weights for final evaluation\n",
    "weights = best_weights\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\nPerforming final evaluation...\")\n",
    "predictions = quantum_classifier(X_test, weights)\n",
    "binary_predictions = (predictions > 0).astype(int)\n",
    "accuracy = accuracy_score(y_test, binary_predictions)\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, binary_predictions))\n",
    "\n",
    "\n",
    "print(\"\\nExecution completed at:\", time.strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('C:\\\\nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data] Error downloading 'wordnet' from\n",
      "[nltk_data]     <https://raw.githubusercontent.com/nltk/nltk_data/gh-\n",
      "[nltk_data]     pages/packages/corpora/wordnet.zip>:   [Errno 28] No\n",
      "[nltk_data]     space left on device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', download_dir='C:\\\\nltk_data')\n",
    "nltk.download('stopwords', download_dir='C:\\\\nltk_data')\n",
    "nltk.download('wordnet', download_dir='C:\\\\nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\dell/nltk_data', 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data', 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data', 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\dell\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'C:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting execution at: 02:26:16\n",
      "Loading dataset...\n",
      "Cleaning and preprocessing data...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\dell/nltk_data'\n    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\dell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet_Text\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Apply text preprocessing\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet_Text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTweet_Text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Remove empty texts after preprocessing\u001b[39;00m\n\u001b[0;32m     56\u001b[0m data \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet_Text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[1], line 35\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     33\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Tokenize\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Remove stopwords and lemmatize\u001b[39;00m\n\u001b[0;32m     37\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\dell/nltk_data'\n    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\dell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Set NLTK data path explicitly to ensure it can find the necessary files\n",
    "nltk.data.path.append('C:\\\\nltk_data')\n",
    "\n",
    "# Download required NLTK data (handling errors if already downloaded)\n",
    "try:\n",
    "    nltk.download('punkt', download_dir='C:\\\\nltk_data')\n",
    "    nltk.download('stopwords', download_dir='C:\\\\nltk_data')\n",
    "    nltk.download('wordnet', download_dir='C:\\\\nltk_data')\n",
    "except LookupError:\n",
    "    print(\"NLTK data already downloaded or path issue\")\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Starting execution at:\", time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "print(\"Loading dataset...\")\n",
    "data = pd.read_csv(r'C:\\Users\\dell\\Desktop\\twitter.csv')\n",
    "data.columns = ['ID', 'Keyword', 'Sentiment', 'Tweet_Text']\n",
    "\n",
    "# Enhanced data cleaning\n",
    "print(\"Cleaning and preprocessing data...\")\n",
    "# Remove rows with missing values\n",
    "data = data.dropna(subset=['Tweet_Text', 'Sentiment'])\n",
    "# Apply text preprocessing\n",
    "data['Tweet_Text'] = data['Tweet_Text'].apply(preprocess_text)\n",
    "# Remove empty texts after preprocessing\n",
    "data = data[data['Tweet_Text'].str.len() > 0]\n",
    "\n",
    "# Balance the dataset\n",
    "print(\"Balancing dataset...\")\n",
    "sentiment_counts = data['Sentiment'].value_counts()\n",
    "min_count = min(sentiment_counts)\n",
    "balanced_data = pd.DataFrame()\n",
    "for sentiment in sentiment_counts.index:\n",
    "    sentiment_data = data[data['Sentiment'] == sentiment]\n",
    "    balanced_data = pd.concat([balanced_data, sentiment_data.sample(n=min_count, random_state=42)])\n",
    "\n",
    "data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"Using {len(data)} balanced samples\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sentiment'] = label_encoder.fit_transform(data['Sentiment'])\n",
    "\n",
    "# Enhanced text vectorization\n",
    "print(\"Vectorizing text...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=8,  # Increased features\n",
    "    ngram_range=(1, 3),  # Added trigrams\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    stop_words='english',\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True  # Apply sublinear scaling\n",
    ")\n",
    "X = vectorizer.fit_transform(data['Tweet_Text']).toarray()\n",
    "y = data['Sentiment'].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Quantum device setup\n",
    "n_qubits = 8  # Increased number of qubits\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "# Enhanced quantum circuit with more layers\n",
    "@qml.qnode(dev)\n",
    "def quantum_circuit(inputs, weights):\n",
    "    # Normalize inputs\n",
    "    inputs = inputs / (np.linalg.norm(inputs) + 1e-10)\n",
    "    \n",
    "    # First rotation layer\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(inputs[i % len(inputs)], wires=i)\n",
    "        qml.RY(weights[i], wires=i)\n",
    "        qml.RZ(weights[i + n_qubits], wires=i)\n",
    "    \n",
    "    # First entanglement layer\n",
    "    for i in range(n_qubits-1):\n",
    "        qml.CNOT(wires=[i, i+1])\n",
    "    qml.CNOT(wires=[n_qubits-1, 0])\n",
    "    \n",
    "    # Second rotation layer\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(weights[i + 2*n_qubits], wires=i)\n",
    "        qml.RY(weights[i + 3*n_qubits], wires=i)\n",
    "        qml.RZ(weights[i + 4*n_qubits], wires=i)\n",
    "    \n",
    "    # Second entanglement layer\n",
    "    for i in range(0, n_qubits-1, 2):\n",
    "        qml.CNOT(wires=[i, i+1])\n",
    "    \n",
    "    # Final rotation layer\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(weights[i + 5*n_qubits], wires=i)\n",
    "        qml.RY(weights[i + 6*n_qubits], wires=i)\n",
    "    \n",
    "    # Measure all qubits and take average\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "# Enhanced classical processing\n",
    "def quantum_classifier(inputs, weights):\n",
    "    predictions = []\n",
    "    batch_size = 16  # Smaller batch size for more frequent updates\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch = inputs[i:i + batch_size]\n",
    "        batch_predictions = np.array([np.mean(quantum_circuit(x, weights)) for x in batch])\n",
    "        predictions.extend(batch_predictions)\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Enhanced cost function\n",
    "def cost(weights, X, y, lambda_reg=0.01):\n",
    "    predictions = quantum_classifier(X, weights)\n",
    "    epsilon = 1e-10\n",
    "    predictions = np.clip((predictions + 1) / 2, epsilon, 1 - epsilon)\n",
    "    # Weighted binary cross-entropy\n",
    "    pos_weight = np.sum(y == 0) / np.sum(y == 1)\n",
    "    loss = -np.mean(pos_weight * y * np.log(predictions) + \n",
    "                   (1 - y) * np.log(1 - predictions))\n",
    "    # L2 regularization\n",
    "    reg_term = lambda_reg * np.sum(weights**2)\n",
    "    return loss + reg_term\n",
    "\n",
    "# Initialize weights\n",
    "np.random.seed(42)\n",
    "n_weights = 7 * n_qubits  # 7 rotation layers\n",
    "weights = np.random.uniform(-0.1, 0.1, size=n_weights)\n",
    "\n",
    "# Training settings\n",
    "opt = qml.AdamOptimizer(stepsize=0.01, beta1=0.9, beta2=0.999)\n",
    "batch_size = 16\n",
    "epochs = 50  # Increased epochs\n",
    "\n",
    "# Training loop with enhanced early stopping\n",
    "print(\"\\nTraining quantum classifier...\")\n",
    "training_start = time.time()\n",
    "best_loss = float('inf')\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "best_weights = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Shuffle training data\n",
    "    shuffle_idx = np.random.permutation(len(X_train))\n",
    "    X_train_shuffled = X_train[shuffle_idx]\n",
    "    y_train_shuffled = y_train[shuffle_idx]\n",
    "    \n",
    "    # Mini-batch training\n",
    "    epoch_losses = []\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train_shuffled[i:i + batch_size]\n",
    "        y_batch = y_train_shuffled[i:i + batch_size]\n",
    "        \n",
    "        weights = opt.step(lambda w: cost(w, X_batch, y_batch), weights)\n",
    "        batch_loss = cost(weights, X_batch, y_batch)\n",
    "        epoch_losses.append(batch_loss)\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Early stopping with best weights saving\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_weights = weights.copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # Validate current accuracy\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        current_preds = quantum_classifier(X_test, weights)\n",
    "        current_preds = (current_preds > 0).astype(int)\n",
    "        current_acc = accuracy_score(y_test, current_preds)\n",
    "        print(f\"Current accuracy: {current_acc * 100:.2f}%\")\n",
    "        \n",
    "        if current_acc > 0.8:  # Early stopping if accuracy > 80%\n",
    "            print(\"Reached target accuracy!\")\n",
    "            break\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "# Use best weights for final evaluation\n",
    "weights = best_weights\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\nPerforming final evaluation...\")\n",
    "final_preds = quantum_classifier(X_test, weights)\n",
    "final_preds = (final_preds > 0).astype(int)\n",
    "final_acc = accuracy_score(y_test, final_preds)\n",
    "print(f\"Final accuracy: {final_acc * 100:.2f}%\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(y_test, final_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
